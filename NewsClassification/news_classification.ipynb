{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 导入并读取文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import jieba\n",
    "import glob\n",
    "## 读取文件列表\n",
    "def get_filelist(data_dir):\n",
    "    \"\"\"\n",
    "    :param data_dir: raw data directory\n",
    "    :return: A list of news file\n",
    "    \"\"\"\n",
    "\n",
    "    file_list = glob.glob(os.path.join(data_dir, '*_*.txt'))\n",
    "    return file_list\n",
    "\n",
    "\n",
    "## 用jieba进行中文分词\n",
    "def words_parsing(file_list):\n",
    "    \"\"\"\n",
    "    :param file_list: A list or news files\n",
    "    :return:\n",
    "        class_label: text label list of each news\n",
    "        words: news record after parsing\n",
    "    \"\"\"\n",
    "    class_label = []\n",
    "    words = []\n",
    "    for f in file_list:\n",
    "        with open(f, 'rb', ) as fh:\n",
    "            each_news = ''.join([l.decode(encoding='utf-8').strip() for l in fh.readlines()])\n",
    "        class_label.append(os.path.basename(f).split('_')[0].lower())\n",
    "        parsed_words = ' '.join(list(jieba.cut(each_news)))\n",
    "        words.append(parsed_words)\n",
    "    return class_label, words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 分词后的语料向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as t2v\n",
    "## 使用tfidf向量化\n",
    "def to_vector_Tfidf(parsed_words):\n",
    "    \"\"\"\n",
    "\n",
    "    :param parsed_words: string list of parsed by jieba\n",
    "    :return: vectors transformed using Tf-IDF algorithms\n",
    "    \"\"\"\n",
    "    vectorizer = t2v.TfidfVectorizer(max_features=10000)\n",
    "    vectors = vectorizer.fit_transform(parsed_words)\n",
    "    return vectors\n",
    "\n",
    "\n",
    "## 使用tf向量化\n",
    "def to_vector_countVectors(parsed_words):\n",
    "    vectorizer = t2v.CountVectorizer(max_df=0.95, min_df=2, max_features=10000)\n",
    "    vectors = vectorizer.fit_transform(parsed_words)\n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 创建wrapper 函数， 整合数据预处理部分\n",
    "def data_prepare(data_dir, vectorizer):\n",
    "    \"\"\"\n",
    "    :param data_dir: raw data director\n",
    "    :return: vectors labels and d_label(label dict)\n",
    "    \"\"\"\n",
    "    file_list = get_filelist(data_dir)\n",
    "    class_label, words = words_parsing(file_list)\n",
    "    vectors = vectorizer(words)\n",
    "    s_label = set(class_label)\n",
    "    d_label = dict(zip(s_label, range(len(s_label))))\n",
    "    labels = [d_label[k] for k in class_label]\n",
    "    return vectors, labels, d_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用两种向量化方式进行向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from cache /var/folders/f4/74s3c6ln0d1dh3k95wlkysvh0000gn/T/jieba.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.146 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## 使用Tfidf进行向量化\n",
    "data1, labels1, label_dict1 = data_prepare(\"data/news\", to_vector_Tfidf)\n",
    "train_data1, test_data1, train_label1, test_label1 = train_test_split(data1, labels1)\n",
    "\n",
    "## 使用tf进行向量化\n",
    "data2, labels2, label_dict2 = data_prepare(\"data/news\", to_vector_countVectors)\n",
    "train_data2, test_data2, train_label2, test_label2 = train_test_split(data1, labels1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 模型选择与优化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 使用逻辑回归作为基线模型，选择最好的向量化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.983944444444, test: 0.968333333333\nRunning time: fit 6.230064868927002, Predict on Train: 0.021807193756103516, Predict on Test: 0.007370948791503906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.982722222222, test: 0.9785\nRunning time: fit 5.664386034011841, Predict on Train: 0.034255027770996094, Predict on Test: 0.011272907257080078\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "def lr_model(train_data, train_label, test_data, test_label, penalty, C):\n",
    "    lr = LogisticRegression(multi_class='multinomial', solver='newton-cg', penalty=penalty, C=C)\n",
    "    train_pred, test_pred, train_score, test_score =_performance(lr, train_data, train_label, test_data, test_label)\n",
    "    return train_pred, test_pred, train_score, test_score\n",
    "\n",
    "def _performance(md, train_data, train_label, test_data, test_label):\n",
    "    fit_begin = time.time()\n",
    "    md.fit(train_data, train_label)\n",
    "    fit_time = time.time() - fit_begin\n",
    "    train_score = md.score(train_data, train_label)\n",
    "    test_score = md.score(test_data, test_label)\n",
    "    train_pred_begin = time.time()\n",
    "    train_pred = md.predict(train_data)\n",
    "    train_pred_time = time.time() - train_pred_begin\n",
    "    test_pred_begin = time.time()\n",
    "    test_pred = md.predict(test_data)\n",
    "    test_pred_time = time.time() - test_pred_begin\n",
    "    print(\"Predict accuracy: train \"+str(train_score) + \", test: \" + str(test_score))\n",
    "    print(\"Running time: fit \" + str(fit_time) + \", Predict on Train: \" + str(train_pred_time) + \", Predict on Test: \" + str(test_pred_time))\n",
    "    return train_pred, test_pred, train_score, test_score\n",
    "\n",
    "## 逻辑回归是我们的基模型， 使用基模型来测试两种向量化方式哪种更好\n",
    "# tifidf\n",
    "_, _, _, _=lr_model(train_data1, train_label1, test_data1, test_label1, penalty='l2', C=1)\n",
    "\n",
    "# tf\n",
    "_, _, _, _ = lr_model(train_data2, train_label2, test_data1, test_label1, penalty='l2', C=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由此可见对于基模型， 使用tf效果更好， 因此后续的模型调优中持续使用tf向量化结果。 后续会持续有这一组数据寻找模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.2 随机森林\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.9985, test: 0.984\nRunning time: fit 3.6753311157226562, Predict on Train: 0.1686561107635498, Predict on Test: 0.05652594566345215\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## 定义一个helper 函数， 方便参数调试\n",
    "def rd_model(train_data, train_label, test_data, test_label, n, depth):\n",
    "    rd = RandomForestClassifier(n_estimators=n, max_depth=depth)\n",
    "    train_pred, test_pred, _, _ =_performance(rd, train_data, train_label, test_data, test_label)\n",
    "    return train_pred, test_pred\n",
    "\n",
    "_, _= rd_model(train_data2, train_label2, test_data1, test_label1, n=10,  depth=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果可见训练精度比比基模型有所提升， 测试也有所提升。但训练和测试有一定差异， 说明模型有些过拟合。 所以后面尝试增大树个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.999611111111, test: 0.989166666667\nRunning time: fit 7.208452939987183, Predict on Train: 0.29341578483581543, Predict on Test: 0.10657191276550293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.999777777778, test: 0.988833333333\nRunning time: fit 10.417667865753174, Predict on Train: 0.43949294090270996, Predict on Test: 0.15072202682495117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.999833333333, test: 0.990333333333\nRunning time: fit 14.381725072860718, Predict on Train: 0.6063001155853271, Predict on Test: 0.20412707328796387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.999888888889, test: 0.990333333333\nRunning time: fit 18.092178106307983, Predict on Train: 0.7303857803344727, Predict on Test: 0.2601778507232666\n"
     ]
    }
   ],
   "source": [
    "_, _= rd_model(train_data2, train_label2, test_data1, test_label1, n=20,  depth=None)\n",
    "\n",
    "_, _= rd_model(train_data2, train_label2, test_data1, test_label1, n=30,  depth=None)\n",
    "\n",
    "_, _= rd_model(train_data2, train_label2, test_data1, test_label1, n=40,  depth=None)\n",
    "\n",
    "_, _= rd_model(train_data2, train_label2, test_data1, test_label1, n=50,  depth=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果可以看到， 测试集预测结果在30棵树的时候已经达到99%, 再增加树的个数，性能提升不明显。 随机森林性能调优到此为止。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 尝试使用深度神经网络。\n",
    "我们看到，基模型的原始性能就不错， 但本质来说逻辑回归就是一个单层神经网络， 所以这里考虑使用层次更多的神经网络来测试结果是否有提升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.999833333333, test: 0.968666666667\nRunning time: fit 18.935806035995483, Predict on Train: 0.019276857376098633, Predict on Test: 0.006644010543823242\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def nn_model(train_data, train_label, test_data, test_label, hidden_layer_sizes = 4, alpha=0.00001):\n",
    "    nn = MLPClassifier(hidden_layer_sizes, activation='relu', alpha=alpha)\n",
    "    train_pred, test_pred, _, _ =_performance(nn, train_data, train_label, test_data, test_label)\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "## 初始使用4个隐藏层网络，加0.00001的L2惩罚项\n",
    "_, _ = nn_model(train_data2, train_label2, test_data2, test_label2, hidden_layer_sizes=4, alpha = 0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果显示测试数据较差， 训练数据特别好， 结论是过拟合。 调优方向增大惩罚项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.999666666667, test: 0.97\nRunning time: fit 21.17419195175171, Predict on Train: 0.01936197280883789, Predict on Test: 0.006600141525268555\n"
     ]
    }
   ],
   "source": [
    "_, _ = nn_model(train_data2, train_label2, test_data2, test_label2, hidden_layer_sizes=4, alpha = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_, _ = nn_model(train_data2, train_label2, test_data2, test_label2, hidden_layer_sizes=4, alpha = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.999611111111, test: 0.972833333333\nRunning time: fit 28.278118133544922, Predict on Train: 0.035012006759643555, Predict on Test: 0.008856058120727539\n"
     ]
    }
   ],
   "source": [
    "_, _ = nn_model(train_data2, train_label2, test_data2, test_label2, hidden_layer_sizes=4, alpha = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.996222222222, test: 0.972666666667\nRunning time: fit 25.884929895401, Predict on Train: 0.01914691925048828, Predict on Test: 0.006716012954711914\n"
     ]
    }
   ],
   "source": [
    "_, _ = nn_model(train_data2, train_label2, test_data2, test_label2, hidden_layer_sizes=4, alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.996777777778, test: 0.9725\nRunning time: fit 22.74985098838806, Predict on Train: 0.03649187088012695, Predict on Test: 0.01559591293334961\n"
     ]
    }
   ],
   "source": [
    "_, _ = nn_model(train_data2, train_label2, test_data2, test_label2, hidden_layer_sizes=10, alpha = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过多次调整惩罚项， 直到测试集性能不再提升后。 又加深了网络， 发现性能依然没有显著提高。 4层网络和alpha为0.01时候达到最佳性能。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后续陆续尝试了, SVM, GN环境下的bayes，运行都很缓慢。 无法再个人电脑上很快算出结果。后尝试用LDA语义模型进行压缩，发现依然很缓慢。于是放弃对原始数据进行压缩。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 adaboost 和 GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with 5trees\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.548944444444, test: 0.544666666667\nRunning time: fit 2.349236011505127, Predict on Train: 0.08429813385009766, Predict on Test: 0.026417016983032227\n\nTest with 10trees\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.701166666667, test: 0.6975\nRunning time: fit 4.555819988250732, Predict on Train: 0.16355681419372559, Predict on Test: 0.055429935455322266\n\nTest with 20trees\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.792444444444, test: 0.789\nRunning time: fit 8.74092698097229, Predict on Train: 0.2732670307159424, Predict on Test: 0.0889279842376709\n\nTest with 40trees\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.824, test: 0.821666666667\nRunning time: fit 17.413145065307617, Predict on Train: 0.5463409423828125, Predict on Test: 0.1847219467163086\n\nTest with 80trees\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.831333333333, test: 0.8255\nRunning time: fit 35.18995809555054, Predict on Train: 1.0784039497375488, Predict on Test: 0.3642890453338623\n\nTest with 160trees\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.851388888889, test: 0.843333333333\nRunning time: fit 69.55302906036377, Predict on Train: 2.13319993019104, Predict on Test: 0.7491791248321533\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "def ada_model(train_data, train_label, test_data, test_label, n):\n",
    "    ada = AdaBoostClassifier(n_estimators=n)\n",
    "    train_pred, test_pred, _, _ =_performance(ada, train_data, train_label, test_data, test_label)\n",
    "    return train_pred, test_pred\n",
    "\n",
    "for n in [5, 10, 20, 40, 80, 160]:\n",
    "    print('Test with ' + str(n) + ' trees')\n",
    "    _, _ = ada_model(train_data2, train_label2, test_data2, test_label2, n)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "结果可以看到，弱分类器个数到达40个开始性能分类准确度提升就比较缓慢， 但是时间开销却增加很快。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 尝试使用GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def gbdt_model(train_data, train_label, test_data, test_label,n):\n",
    "    gbdt = GradientBoostingClassifier()\n",
    "    train_pred, test_pred, _, _ =_performance(gbdt, train_data, train_label, test_data, test_label)\n",
    "    return train_pred, test_pred\n",
    "\n",
    "## 结果跑的太慢， 放弃这个方案。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "经过以上测试， 找到了三个还说的过去的分类器和配套参数\n",
    "使用tf向量化模型。\n",
    "1. 逻辑回归，配套L2惩罚项。\n",
    "2. 40个分类器的随机森林。\n",
    "3. 四层神经网络， 配合l2惩罚项和0.01的惩罚系数\n",
    "总体对比如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base model:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.982722222222, test: 0.9785\nRunning time: fit 7.0465850830078125, Predict on Train: 0.02595996856689453, Predict on Test: 0.006911039352416992\nRandom Forest with 50 trees:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.999888888889, test: 0.990666666667\nRunning time: fit 17.748448848724365, Predict on Train: 0.7399890422821045, Predict on Test: 0.25372982025146484\nFNN with 4 hiden layer:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict accuracy: train 0.999777777778, test: 0.971833333333\nRunning time: fit 30.017409801483154, Predict on Train: 0.019389867782592773, Predict on Test: 0.006413936614990234\n"
     ]
    }
   ],
   "source": [
    "## 基模型\n",
    "print('base model:')\n",
    "_, _, _, _ = lr_model(train_data2, train_label2, test_data1, test_label1, penalty='l2', C=1)\n",
    "\n",
    "## 30棵树的随机森林\n",
    "print('Random Forest with 50 trees:')\n",
    "_, _= rd_model(train_data2, train_label2, test_data1, test_label1, n=50,  depth=None)\n",
    "\n",
    "## 四层神经网络\n",
    "print('FNN with 4 hiden layer:')\n",
    "_, _ = nn_model(train_data2, train_label2, test_data2, test_label2, hidden_layer_sizes=4, alpha = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回顾一下， 最初的文本向量化在精确度提升上起到很大作用。 不同的向量化对结果影响巨大。 \n",
    "总体而言我们的基模型表现应该是最平衡的， 其次是随机森林算法。不过却显示预测的事件较长接近1秒钟。 \n",
    "深度神经网络训练集表现远远大于测试集表现， 考虑是不是需要进一步增加数据提高模型的泛化能力。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
