{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 无框架多层神经网络识别猫咪图片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此项目目的在熟练掌握多层全连接网络的基本原理。 使用python/numpy手工实现多层神经网络。 数据集使用两组h5格式的图片数据集。\n",
    "需要导入的库如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py    # 用于导入h5数据\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(121)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 多层神经网络每一层输入输出维度关系\n",
    "由于需要从头开始实现神经网络， 在争相传播和反向传播时， 需要矩阵运算维度保持匹配。 在开工支之前先把维度弄清楚。\n",
    "比如输入层X维度是$(n^{[1]}=12288,m^{[1]}=209)$, 也就是特征有12288维， 样品数209个。具体维度如下：\n",
    "\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "\t<tr>\n",
    "\t\t<th>网络层数</th>\n",
    "\t\t<th>W维度</th>\n",
    "\t\t<th>b维度</th>\n",
    "\t\t<th>Z值</th>\n",
    "\t\t<th>A维度</th>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Layer 1</td>\n",
    "\t\t<td>$(n^{[1]}, 12288)$</td>\n",
    "\t\t<td>$(n^{[1]}, 1)$</td>\n",
    "\t\t<td>$Z^{[1]}=W^{[1]}X + b^{[1]}$</td>\n",
    "\t\t<td>$(n^{[1]}, 209)$</td>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "\t\t<td>Layer 2</td>\n",
    "\t\t<td>$(n^{[2]}, n^{[1]})$</td>\n",
    "\t\t<td>$(n^{[2]}, 1)$</td>\n",
    "\t\t<td>$Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$</td>\n",
    "\t\t<td>$(n^{[2]}, 209)$</td>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "        \t<td>$\\vdots$</td> \n",
    "        \t<td>$\\vdots$</td> \n",
    "        \t<td>$\\vdots$</td> \n",
    "        \t<td>$\\vdots$</td> \n",
    "        \t<td>$\\vdots$</td> \n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "\t\t<td>Layer L-1</td>\n",
    "\t\t<td>$(n^{[L-1]}, n^{[L-2]})$</td>\n",
    "\t\t<td>$(n^{[L-1]}, 1)$</td>\n",
    "\t\t<td>$Z^{[L-1]}=W^{[L-1]}A^{[L-2]} + b^{[L-2]}$</td>\n",
    "\t\t<td>$(n^{[L-1]}, 209)$</td>\n",
    "\t</tr>\n",
    "\t\n",
    "\t<tr>\n",
    "\t\t<td>Layer L</td>\n",
    "\t\t<td>$(n^{[L]}, n^{[L-1]})$</td>\n",
    "\t\t<td>$(n^{[L]}, 1)$</td>\n",
    "\t\t<td>$Z^{[L]}=W^{[L-1]}A^{[L-1]} + b^{[L-1]}$</td>\n",
    "\t\t<td>$(n^{[L]}, 209)$</td>\n",
    "\t</tr>\n",
    "\t\n",
    "</table>\n",
    "\n",
    "网络基本构成如下：\n",
    "[Linear --> RELU](L-1层) --> [Linear --> Sigmoid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 向后传播（Backward Propagation）\n",
    "向后传播涉及一系列链式求导。中间L-1层的一系列梯度如下。 \n",
    "\n",
    "激活函数的梯度\n",
    "\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$\n",
    "\n",
    "线性函数梯度\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "\n",
    "而第L层的dAL如下：\n",
    "$$dAL=\\frac{\\partial{J}}{\\partial{A^{[L]}}}=-(\\frac{Y}{A^{[L]}}-\\frac{1-Y}{1-A^{[L]}})$$\n",
    "\n",
    "\n",
    "#### 4.1 激活函数梯度\n",
    "为了后面计算起来方便，先来写激活函数的梯度\n",
    "**Relu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 权值初始化\n",
    "* 对于神经网络而言， 不能把多神经元的W全部初始化为0， 如果的这样的话， 训练出来每个神经元结果完全一样。 效果等同于一个逻辑回归函数。 \n",
    "* 为了保证多层变换以后不至于输出结果过大导致梯度爆炸，初始权值尽量小一些。 使用np.random.randn(shape)*0.01初始化W\n",
    "* b可以初始位0， np.zeros(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_layer_init(layer_dims_list):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param \n",
    "        list_layer_dims -- python list of dims of each layer. Each element is a int number of hidden unit\n",
    "    :return: \n",
    "        param_weights: python dict of W,b for each layers\n",
    "    \"\"\"\n",
    "    L = len(layer_dims_list)\n",
    "    param_weights = {}\n",
    "    np.random.seed(121)\n",
    "    for l in range(1, L):\n",
    "        W = np.random.randn(layer_dims_list[l],layer_dims_list[l-1]) * 0.01\n",
    "        b = np.zeros([layer_dims_list[l], 1])\n",
    "        param_weights['W'+str(l)] = W\n",
    "        param_weights['b'+str(l)] = b\n",
    "    return param_weights      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设初始一个3层网络， 输入特征10维，第二层，3个hidden unit, 输出层1维。 隐藏层和输出层的参数权重维度应该是\n",
    "W1(3,10), b1(3,1), W2(1, 3), b2(1, 1), 测试一下上述函数。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1\n(3, 10)\nb1\n(3, 1)\n\nW2\n(1, 3)\nb2\n(1, 1)\n"
     ]
    }
   ],
   "source": [
    "test_dims_list = [10, 3, 1]\n",
    "test_params_weights = l_layer_init(test_dims_list)\n",
    "print('W1')\n",
    "print(test_params_weights['W1'].shape)\n",
    "print('b1')\n",
    "print(test_params_weights['b1'].shape)\n",
    "print('\\nW2')\n",
    "print(test_params_weights['W2'].shape)\n",
    "print('b2')\n",
    "print(test_params_weights['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 向前传播Forward Propagation\n",
    "向前传播包括两部分变换\n",
    "* 线性变换\n",
    "* 非线性激活函数变换\n",
    "因此需要把需要的激活函数定义出来。 这个项目中， 隐藏层使用relu函数， 输出层牵扯到分类问题，使用sigmoid函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    cache = Z\n",
    "    A = np.maximum(0, Z)\n",
    "    return A, cache\n",
    "\n",
    "def sigmoid(Z):\n",
    "    cache = Z\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激活函数定义完毕后，准备定义forward_propagation函数\n",
    "#### 2.2 定义向前传播函数\n",
    "* 第一层网络， $A^{[0]}=X$\n",
    "* 前L-1层网络，$Z^{[l]}=W^{[l]}A^{[l-1]} + b^{[l]}，A^{[l]}=Relu(Z^{[l]})$\n",
    "* 第L层网络， $Z^{[L]}=W^{[L]}A^{[L-1]} + b^{[L]}，A^{[L]}=sigmoid(Z^{[L]})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == 'relu':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def forward_propagation(X, param_weights):\n",
    "    \"\"\"\n",
    "    :param param_weights: python dict of parameters weights\n",
    "    :param X: input matrix\n",
    "    \n",
    "    :return: \n",
    "    AL -- output layer activation value\n",
    "    caches -- list of caches containing Z value for each layer\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(param_weights)//2\n",
    "    \n",
    "    ## impelment [linear --> relu]*(L-1) --> [linear --> sigmoid]\n",
    "    \n",
    "    ## L-1 layers\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        Wl, bl = param_weights['W' + str(l)], param_weights['b'+str(l)]\n",
    "        A, cache = linear_activation_forward(A_prev, Wl, bl, 'relu')\n",
    "        caches.append(cache)\n",
    "    WL, bL = param_weights['W' + str(L)], param_weights['b' + str(L)]\n",
    "    AL, cache = linear_activation_forward(A, WL, bL, 'sigmoid')\n",
    "    caches.append(cache)\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cost 函数\n",
    "由于是二分类问题， 因此使用交叉熵来描述损失函数\n",
    "\n",
    "$J(W,b)=\\frac{1}{m}\\sum_{i=1}^{m}((y^{(i)}log(a^{[L](i)})+(1-y^{(i)})log(1-a^{[L](i)}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(AL, Y):\n",
    "    \"\"\"\n",
    "    :param AL: output layer activation value\n",
    "    :param Y: label values \n",
    "    :return: entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    J = 1/m * np.dot(Y, np.log(AL.T)) + np.dot(1-Y, np.log(1-AL.T))\n",
    "    J = np.squeeze(J)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 向后传播（Backward Propagation）\n",
    "向后传播涉及一系列链式求导。中间L-1层的一系列梯度如下。 \n",
    "\n",
    "激活函数的梯度\n",
    "\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$\n",
    "\n",
    "线性函数梯度\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "\n",
    "而第L层的dAL如下：\n",
    "$$dAL=\\frac{\\partial{J}}{\\partial{A^{[L]}}}=-(\\frac{Y}{A^{[L]}}-\\frac{1-Y}{1-A^{[L]}})$$\n",
    "\n",
    "\n",
    "#### 4.1 激活函数梯度\n",
    "为了后面计算起来方便，先来写激活函数的梯度\n",
    "**Relu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z<=0] = 0\n",
    "    return dZ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**sigmoid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, Z):\n",
    "    a = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * a * (1-a)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 线性好书梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 一层网络中的backward\n",
    "上面两步用到的函数连起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dZ, linear_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 完整实现Backward Propagation\n",
    "把上面的函数串起来，放到L层网络里面，完整实现BP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Initialization of dAL\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    current_cache = caches[-1]\n",
    "    grads[\"A\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = \\\n",
    "        linear_activation_backward(dAL, current_cache,'sigmoid')\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = \\\n",
    "            linear_activation_backward(grads[\"dA\"+str(l+2)], current_cache, 'relu')\n",
    "        grads['dA' + str(l + 1)] = dA_prev_temp\n",
    "        grads['dW' + str(l + 1)] = dW_temp\n",
    "        grads['db' + str(l + 1)] = db_temp\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 梯度下降法更新权值\n",
    "\n",
    "$W^{[l]}=W^{[l]}-\\alpha{dW^{[l]}}$\n",
    "\n",
    "$b^{[l]}=b^{[l]}-\\alpha{db^{[l]}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(param_weights, grads, learning_rate):\n",
    "    L = len(param_weights)\n",
    "    for l in range(1, L+1):\n",
    "        param_weights['W' + str(l)] -= learning_rate*grads['dW' + str(l)]\n",
    "        param_weights['b' + str(l)] -= learning_rate*grads['db' + str(l)]\n",
    "    return param_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. 最终L层深度神经网络函数\n",
    "def L_layer_model(X, Y, layers_dims_list, learning_rate = 0.001, num_iterations = 3000):\n",
    "    np.random.seed(121)\n",
    "    costs = []\n",
    "    params_weights = l_layer_init(layers_dims_list)\n",
    "    for i in range(num_iterations):\n",
    "        AL, caches = forward_propagation(X, params_weights)\n",
    "        costs.append(cost(AL, Y))\n",
    "        grads = backward_propagation(AL, Y, caches)\n",
    "        params_weights = update_parameters(params_weights, grads, learning_rate)\n",
    "        if i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, costs[-1]))\n",
    "    \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return params_weights\n",
    "\n",
    "def predict(X, y, params_weight):\n",
    "    m = X.shape[1]\n",
    "    p = np.zeros([1, m])\n",
    "    prob, _ = forward_propagation(X, params_weight)\n",
    "    \n",
    "    for i in range(0, prob.shape[1]):\n",
    "        if prob[0, i]> 0.5:\n",
    "            p[0, i] = 1\n",
    "        else:\n",
    "            p[0, i] = 0\n",
    "    print(\"Accuracy: \" + str(np.sum((p==y)/m)))\n",
    "    return p\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 导入数据进行实验\n",
    "#### 7.1 导入h5格式的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (File signature not found)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6cfe4c3477d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m## load train dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/train_catvnoncat.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# train_set_x_raw = np.array(train_dataset[\"train_set_X\"][:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# train_set_y_raw = np.array(train_dataset[\"train_set_y\"][:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/MIAOKUI/anaconda3/lib/python3.5/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/MIAOKUI/anaconda3/lib/python3.5/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/ilan/minonda/conda-bld/h5py_1490026960179/work/h5py/_objects.c:2846)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/ilan/minonda/conda-bld/h5py_1490026960179/work/h5py/_objects.c:2804)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open (/Users/ilan/minonda/conda-bld/h5py_1490026960179/work/h5py/h5f.c:2123)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (File signature not found)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import h5py\n",
    "## load train dataset\n",
    "train_dataset = h5py.File('datasets/train_catvnoncat.h5', 'r')\n",
    "# train_set_x_raw = np.array(train_dataset[\"train_set_X\"][:])\n",
    "# train_set_y_raw = np.array(train_dataset[\"train_set_y\"][:])\n",
    "# ## load test dataset\n",
    "# test_dataset = h5py.File('datasets/test_catvnoncat.h5', 'r')\n",
    "# test_set_x_raw = np.array(train_dataset[\"test_set_X\"][:])\n",
    "# test_set_y_raw = np.array(train_dataset[\"test_set_y\"][:])\n",
    "# clases = np.array(test_dataset[\"list_classes\"][:])\n",
    "# \n",
    "# train_set_y_raw = train_set_y_raw.reshape((1, train_set_y_raw.shape[0]))\n",
    "# test_set_y_raw = test_set_y_raw.reshape((1, test_set_y_raw.shape[0]))\n",
    "# print(train_set_y_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/MIAOKUI/Documents/机器学习/LearningAI/CatRecognition'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
