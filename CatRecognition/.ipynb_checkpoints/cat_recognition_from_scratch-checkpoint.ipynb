{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 无框架多层神经网络识别猫咪图片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此项目目的在熟练掌握多层全连接网络的基本原理。 使用python/numpy手工实现多层神经网络。 数据集使用两组h5格式的图片数据集。\n",
    "需要导入的库如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py    # 用于导入h5数据\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(121)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 多层神经网络每一层输入输出维度关系\n",
    "由于需要从头开始实现神经网络， 在争相传播和反向传播时， 需要矩阵运算维度保持匹配。 在开工支之前先把维度弄清楚。\n",
    "比如输入层X维度是$(n^{[1]}=12288,m^{[1]}=209)$, 也就是特征有12288维， 样品数209个。具体维度如下：\n",
    "\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "\t<tr>\n",
    "\t\t<th>网络层数</th>\n",
    "\t\t<th>W维度</th>\n",
    "\t\t<th>b维度</th>\n",
    "\t\t<th>Z值</th>\n",
    "\t\t<th>A维度</th>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Layer 1</td>\n",
    "\t\t<td>$(n^{[1]}, 12288)$</td>\n",
    "\t\t<td>$(n^{[1]}, 1)$</td>\n",
    "\t\t<td>$Z^{[1]}=W^{[1]}X + b^{[1]}$</td>\n",
    "\t\t<td>$(n^{[1]}, 209)$</td>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "\t\t<td>Layer 2</td>\n",
    "\t\t<td>$(n^{[2]}, n^{[1]})$</td>\n",
    "\t\t<td>$(n^{[2]}, 1)$</td>\n",
    "\t\t<td>$Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$</td>\n",
    "\t\t<td>$(n^{[2]}, 209)$</td>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "        \t<td>$\\vdots$</td> \n",
    "        \t<td>$\\vdots$</td> \n",
    "        \t<td>$\\vdots$</td> \n",
    "        \t<td>$\\vdots$</td> \n",
    "        \t<td>$\\vdots$</td> \n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "\t\t<td>Layer L-1</td>\n",
    "\t\t<td>$(n^{[L-1]}, n^{[L-2]})$</td>\n",
    "\t\t<td>$(n^{[L-1]}, 1)$</td>\n",
    "\t\t<td>$Z^{[L-1]}=W^{[L-1]}A^{[L-2]} + b^{[L-2]}$</td>\n",
    "\t\t<td>$(n^{[L-1]}, 209)$</td>\n",
    "\t</tr>\n",
    "\t\n",
    "\t<tr>\n",
    "\t\t<td>Layer L</td>\n",
    "\t\t<td>$(n^{[L]}, n^{[L-1]})$</td>\n",
    "\t\t<td>$(n^{[L]}, 1)$</td>\n",
    "\t\t<td>$Z^{[L]}=W^{[L-1]}A^{[L-1]} + b^{[L-1]}$</td>\n",
    "\t\t<td>$(n^{[L]}, 209)$</td>\n",
    "\t</tr>\n",
    "\t\n",
    "</table>\n",
    "\n",
    "网络基本构成如下：\n",
    "[Linear --> RELU](L-1层) --> [Linear --> Sigmoid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 权值初始化\n",
    "* 对于神经网络而言， 不能把多神经元的W全部初始化为0， 如果的这样的话， 训练出来每个神经元结果完全一样。 效果等同于一个逻辑回归函数。 \n",
    "* 为了保证多层变换以后不至于输出结果过大导致梯度爆炸，初始权值尽量小一些。 使用np.random.randn(shape)*0.01初始化W\n",
    "* b可以初始位0， np.zeros(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l_layer_init(layer_dims_list):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param \n",
    "        list_layer_dims -- python list of dims of each layer. Each element is a int number of hidden unit\n",
    "    :return: \n",
    "        param_weights: python dict of W,b for each layers\n",
    "    \"\"\"\n",
    "    L = len(layer_dims_list)\n",
    "    param_weights = {}\n",
    "    np.random.seed(121)\n",
    "    for l in range(1, L):\n",
    "        W = np.random.randn(layer_dims_list[l],layer_dims_list[l-1]) * 0.01\n",
    "        b = np.zeros([layer_dims_list[l], 1])\n",
    "        param_weights['W'+str(l)] = W\n",
    "        param_weights['b'+str(l)] = b\n",
    "    return param_weights      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设初始一个3层网络， 输入特征10维，第二层，3个hidden unit, 输出层1维。 隐藏层和输出层的参数权重维度应该是\n",
    "W1(3,10), b1(3,1), W2(1, 3), b2(1, 1), 测试一下上述函数。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1\n",
      "(3, 10)\n",
      "b1\n",
      "(3, 1)\n",
      "\n",
      "W2\n",
      "(1, 3)\n",
      "b2\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "test_dims_list = [10, 3, 1]\n",
    "test_params_weights = l_layer_init(test_dims_list)\n",
    "print('W1')\n",
    "print(test_params_weights['W1'].shape)\n",
    "print('b1')\n",
    "print(test_params_weights['b1'].shape)\n",
    "print('\\nW2')\n",
    "print(test_params_weights['W2'].shape)\n",
    "print('b2')\n",
    "print(test_params_weights['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 向前传播Forward Propagation\n",
    "向前传播包括两部分变换\n",
    "* 线性变换\n",
    "* 非线性激活函数变换\n",
    "因此需要把需要的激活函数定义出来。 这个项目中， 隐藏层使用relu函数， 输出层牵扯到分类问题，使用sigmoid函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    return A\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激活函数定义完毕后，准备定义forward_propagation函数\n",
    "#### 2.2 定义向前传播函数\n",
    "* 第一层网络， $A^{[0]}=X$\n",
    "* 前L-1层网络，$Z^{[l]}=W^{[l]}A^{[l-1]} + b^{[l]}，A^{[l]}=Relu(Z^{[l]})$\n",
    "* 第L层网络， $Z^{[L]}=W^{[L]}A^{[L-1]} + b^{[L]}，A^{[L]}=sigmoid(Z^{[L]})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(param_weights, X):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
